{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e4931b",
   "metadata": {},
   "source": [
    "# A confusion matrix is a simple yet powerful tool used to evaluate the performance of a classification model. It provides a summary of the predictions made by the model compared to the actual labels of a dataset. Let's break it down into its components:\n",
    "\n",
    "\n",
    "(1). True Positives (TP): These are the cases where the model correctly predicts a positive class (e.g., \"cat\") and the actual label is also positive.\n",
    "    \n",
    "\n",
    "(2). True Negatives (TN): These are the cases where the model correctly predicts a negative class (e.g., \"not cat\") and the actual label is also negative.\n",
    "    \n",
    "\n",
    "(3). False Positives (FP): These are the cases where the model incorrectly predicts a positive class (e.g., \"cat\") but the actual label is negative (e.g., \"not cat\"). This is also known as a Type I error.\n",
    "    \n",
    "\n",
    "(4). False Negatives (FN): These are the cases where the model incorrectly predicts a negative class (e.g., \"not cat\") but the actual label is positive (e.g., \"cat\"). This is also known as a Type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e0509",
   "metadata": {},
   "source": [
    "# Each cell of the matrix represents the count or number of occurrences for that particular combination of predicted and actual classes.\n",
    "\n",
    "\n",
    "Using the information from the confusion matrix, we can calculate various performance metrics, such as:\n",
    "    \n",
    "\n",
    "(1). Accuracy: It measures the overall correctness of the model's predictions and is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "    \n",
    "\n",
    "(2). Precision: It quantifies the model's ability to correctly identify positive cases and is calculated as TP / (TP + FP). It is also referred to as the positive predictive value.\n",
    "    \n",
    "\n",
    "(3). Recall (Sensitivity or True Positive Rate): It measures the model's ability to identify all positive cases correctly and is calculated as TP / (TP + FN). It is also referred to as the hit rate or sensitivity.\n",
    "    \n",
    "\n",
    "(4). Specificity: It measures the model's ability to correctly identify negative cases and is calculated as TN / (TN + FP). It is also referred to as the true negative rate.\n",
    "    \n",
    "    \n",
    "\n",
    "(5). F1 score: It combines precision and recall into a single metric and is calculated as 2 * (Precision * Recall) / (Precision + Recall). It provides a balanced measure of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c1f2b9",
   "metadata": {},
   "source": [
    "# (1). Precision: \n",
    "    Precision is a measure of how many correctly predicted positive instances are actually positive. It focuses on the quality of positive predictions.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Precision is useful when you want to minimize false positive predictions. For example, in spam email detection, you want to make sure that emails classified as spam are indeed spam, even if it means some legitimate emails are mistakenly marked as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0238280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_score\n",
    "\n",
    "# Assuming y_true and y_pred are the true and predicted labels respectively\n",
    "# precision = precision_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13b06d",
   "metadata": {},
   "source": [
    "# (2). Recall (Sensitivity or True Positive Rate):\n",
    "    Recall measures the proportion of actual positive instances that are correctly predicted. It focuses on the ability to find all positive instances.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "\n",
    "Recall is useful when you want to minimize false negative predictions. For example, in a disease diagnosis model, you want to identify all individuals with the disease, even if it means some healthy individuals are incorrectly classified as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c2908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import recall_score\n",
    "\n",
    "# Assuming y_true and y_pred are the true and predicted labels respectively\n",
    "# recall = recall_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525ffcd",
   "metadata": {},
   "source": [
    "# (3). F1 score: \n",
    "    The F1 score combines precision and recall into a single metric. It provides a balance between precision and recall, giving equal weight to both. It is useful when you want to find a trade-off between precision and recall.\n",
    "\n",
    "F1 score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8392a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming y_true and y_pred are the true and predicted labels respectively\n",
    "# f1 = f1_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3113638",
   "metadata": {},
   "source": [
    "# (4). Specificity (True Negative Rate): \n",
    "    Specificity measures the proportion of actual negative instances that are correctly predicted as negative. It focuses on the ability to identify negative instances accurately.\n",
    "\n",
    "Specificity = TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a436a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def specificity_score(y_true, y_pred):\n",
    "#     tn = sum((y_true == 0) & (y_pred == 0))\n",
    "#     fp = sum((y_true == 0) & (y_pred == 1))\n",
    "#     return tn / (tn + fp)\n",
    "\n",
    "# # Assuming y_true and y_pred are the true and predicted labels respectively\n",
    "# specificity = specificity_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcfffdd",
   "metadata": {},
   "source": [
    "# The choice of which metric to use depends on the specific problem and the importance of false positives and false negatives. For example, in fraud detection, you want to minimize false positives (precision), while in cancer screening, you want to minimize false negatives (recall). The F1 score provides a balanced measure when both precision and recall are equally important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a279929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
