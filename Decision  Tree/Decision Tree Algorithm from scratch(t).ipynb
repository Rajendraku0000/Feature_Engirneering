{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51178704",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm \n",
    "\n",
    "The decision tree algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It creates a model in the form of a tree structure that represents a series of decisions and their possible consequences. Each internal node of the tree represents a feature or attribute, each branch represents a decision rule, and each leaf node represents the outcome or prediction.\n",
    "\n",
    "Decision trees are widely used in various domains and applications, such as:\n",
    "\n",
    "(1). Classification problems: Decision trees can be used for classification tasks, where the goal is to assign an input instance to one of the predefined classes. For example, you can use a decision tree to classify whether an email is spam or not.\n",
    "    \n",
    "\n",
    "(2).Regression problems: Decision trees can also be used for regression tasks, where the goal is to predict a continuous value. For example, you can use a decision tree to predict the price of a house based on its features.\n",
    "    \n",
    "\n",
    "(3).Decision support systems: Decision trees can be used to build decision support systems that help in decision-making processes by providing a clear and interpretable set of rules.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6982047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9475813",
   "metadata": {},
   "source": [
    "# example -2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f935b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7735982966643009\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('telecom.csv')\n",
    "\n",
    "# Select features and target variable\n",
    "features = data.drop('Churn', axis=1)\n",
    "target = data['Churn']\n",
    "\n",
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed45ec3",
   "metadata": {},
   "source": [
    "# These are some scenerio where decision tree Algorithm does not give best result \n",
    "\n",
    "(1).High-dimensional data: Decision trees can struggle with high-dimensional datasets as the number of features increases. The tree may become overly complex, leading to overfitting or difficulties in finding meaningful splits. In such cases, feature selection or dimensionality reduction techniques may be beneficial.\n",
    "    \n",
    "\n",
    "(2).Linearly separable data: If the data is linearly separable, linear models such as logistic regression or linear SVM may provide better performance and simplicity compared to decision trees.\n",
    "    \n",
    "\n",
    "(3).Continuous target variables with complex relationships: Decision trees may not capture complex relationships between features and continuous target variables as effectively as other algorithms like neural networks or ensemble methods (e.g., random forests or gradient boosting).\n",
    "    \n",
    "\n",
    "(4).Imbalanced datasets: Decision trees can be biased towards dominant classes in imbalanced datasets, especially when using accuracy as the evaluation metric. In such cases, it may be necessary to balance the dataset or use different evaluation metrics such as precision, recall, or F1-score.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b889f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
